{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPs8I0s6icpNivqsFMSHvyt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mguelgui/picProcessCNN/blob/main/CNN_Part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifjx0YRVFaT_"
      },
      "outputs": [],
      "source": [
        "#################################################################\n",
        "### Schritt 1: a) Embeddings extrahieren                      ###\n",
        "###            b) Ähnlichkeit zwischen Vogelbildern berechnen ###\n",
        "###            c) Ergebnisse in Ähnlichkeitsmatrix speichern  ###\n",
        "#################################################################\n",
        "\n",
        "import os                                               # (kann man weglassen, wenn alles auf einmal gerunt wird)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image        # image-Modul enthält Funktionen zur Vorverarbeitung von Bildern zB. in ein CNN-taugliches Format\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # Zum Berechnen der Kosinus-Ähnlichkeit\n",
        "import numpy as np                                      # Zur Arbeit mit Arrays/Matrizen etc.\n",
        "import pandas as pd                                     # Zum Export von Daten in CSV-Dateien\n",
        "\n",
        "ordner_pfad = '/content/Tierbilder' # Pfad zum Ordner festgelegt, in dem sich die Bilder befinden\n",
        "tierbilder = os.listdir(ordner_pfad) # Listet alle Dateien im Ordner ordner_pfad auf und speichert sie in der Liste \"tierbilder\"\n",
        "\n",
        "\n",
        "### Funktion zum Extrahieren von Features mit mehreren vortrainierten Keras-Modellen ###\n",
        "\n",
        "def extract_features_keras(model, preprocess_input, target_size, ordner_pfad):\n",
        "    # Definiert eine Funktion `extract_features_keras`, die die Merkmale von Bildern in einem Ordner extrahiert.\n",
        "    # Die Funktion nimmt vier Argumente entgegen: `model` (ein vortrainiertes Keras-Modell), `preprocess_input` (eine Funktion zur Bildvorverarbeitung),\n",
        "    # `target_size` (die Zielgröße für die Bildskalierung, z. B. (224, 224)) und `ordner_pfad` (der Pfad zum Ordner mit den Bildern)\n",
        "    dateien = sorted(os.listdir(ordner_pfad))     # Liest die Liste der Dateien im angegebenen `ordner_pfad` und sortiert sie alphabetisch\n",
        "    features = []\n",
        "    for datei in dateien:\n",
        "        bild_pfad = os.path.join(ordner_pfad, datei)\n",
        "        img = image.load_img(bild_pfad, target_size=target_size)\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = preprocess_input(img_array)\n",
        "        img_array = tf.expand_dims(img_array, axis=0)\n",
        "        features.append(model.predict(img_array))\n",
        "    return np.concatenate(features, axis=0)\n",
        "# Bilder werden geladen, skaliert, in numpy-arrays umgewandelt; mit preprocess.input normalisiert (= an jeweilige CNN angepasst)\n",
        "# Batch-Dimension wird mit tf.expand_dims zu jedem Bild hinzugefügt, damit jedes einzeln als Batch behandelt wird und nicht alle zusammen\n",
        "# Durchlauf durch die verschiedenen Modelle und Rückgabe der Vektoren\n",
        "\n",
        "\n",
        "### Erstellen eines Dictionarys aus Keras Modellen und deren Preprocessing-Funktionen etc. ###\n",
        "\n",
        "keras_models = {\n",
        "    'ResNet50': (tf.keras.applications.ResNet50(weights='imagenet', include_top=False, pooling='avg'),\n",
        "                 tf.keras.applications.resnet50.preprocess_input, (224, 224)),\n",
        "    'VGG16': (tf.keras.applications.VGG16(weights='imagenet', include_top=False, pooling='avg'),\n",
        "              tf.keras.applications.vgg16.preprocess_input, (224, 224)),\n",
        "    'InceptionV3': (tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, pooling='avg'),\n",
        "                    tf.keras.applications.inception_v3.preprocess_input, (299, 299)),\n",
        "    'DenseNet121': (tf.keras.applications.DenseNet121(weights='imagenet', include_top=False, pooling='avg'),\n",
        "                    tf.keras.applications.densenet.preprocess_input, (224, 224))\n",
        "}\n",
        "# Es wird jeweils das vortrainierte Modell mit den imagenet-Gewichten geladen;\n",
        "# include_top=False bedeutet, dass die oberen Schichten (Klassifikationsschicht) entfernt werden, sodass nur die Convolutional-Layer (Merkmalextraktionsschichten) übrig bleiben\n",
        "# pooling='avg' fügt eine durchschnittliche Pooling-Schicht hinzu, um den Ausgabe-Feature-Vektor zu reduzieren\n",
        "# Danach kommt die spezifische Vorverarbeitungsfunktion und die Zielgröße der Bilder\n",
        "\n",
        "\n",
        "### Extrahieren und speichern der Feature-Embeddings und Erstellen von Ähnlichkeitsmatrizen ###\n",
        "\n",
        "# Iteration über das Dictionary mit der zuvor definierten Funktion zur Feature-Extraktion\n",
        "for model_name, (model, preprocess_input, target_size) in keras_models.items():\n",
        "    features = extract_features_keras(model, preprocess_input, target_size, ordner_pfad)\n",
        "\n",
        "    # Export der Feature-Embeddings in eine CSV-Datei\n",
        "    features_df = pd.DataFrame(features)\n",
        "    features_df.to_csv(f'embeddings_{model_name}_2.csv', index=False)\n",
        "\n",
        "    # Berechnen der Ähnlichkeitsmatrix (Berechnet für jeden Merkmalsvektor im Array features die Kosinus-Ähnlichkeit zu jedem anderen Vektor)\n",
        "    similarity_matrix_2 = cosine_similarity(features)\n",
        "\n",
        "    # Export der Ähnlichkeitsmatrix in eine CSV-Datei\n",
        "    df = pd.DataFrame(similarity_matrix_2)\n",
        "    df.to_csv(f'similarity_matrix_{model_name}_2.csv', index=False)\n",
        "\n",
        "    print(f\"{model_name} similarity matrix:\") # Kann man weglassen, ist nur zum Prüfen des aktuellen Stands und Anschauen der Matrizen\n",
        "    print(similarity_matrix_2)\n",
        "\n",
        "\n",
        "### ACHTUNG: Hier wird Kosine-Korrelation verwendet, um zu beurteilen, wie ähnlich zwei Bilder im Merkmalsraum sind, unabhängig von der absoluten Größe der Merkmalsvektoren.\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "### Schritt 2: Korrelieren der Ähnlichkeitsmatrizen untereinander ###\n",
        "#####################################################################\n",
        "\n",
        "# Wieder Dictionary erstellen zum Zugreifen auf die CSVs\n",
        "similarity_matrices = {\n",
        "    'ResNet50': pd.read_csv('similarity_matrix_ResNet50_2.csv'),\n",
        "    'VGG16': pd.read_csv('similarity_matrix_VGG16_2.csv'),\n",
        "    'InceptionV3': pd.read_csv('similarity_matrix_InceptionV3_2.csv'),\n",
        "    'DenseNet121': pd.read_csv('similarity_matrix_DenseNet121_2.csv')\n",
        "}\n",
        "\n",
        "# Korrelation der Ähnlichkeitsmatrizen berechnen\n",
        "correlations = {}\n",
        "for name1, mat1 in similarity_matrices.items():\n",
        "    for name2, mat2 in similarity_matrices.items():\n",
        "        if name1 < name2:                     # Sicherstellen, dass jedes Modellpaar nur einmal betrachtet wird\n",
        "            mat2 = mat2.set_index(mat1.index) # Sicherstellen, dass die beiden Ähnlichkeitsmatrizen mat1 und mat2 denselben Index haben\n",
        "            corr = mat1.stack().corr(mat2.stack())\n",
        "            correlations[f'{name1} vs {name2}'] = corr\n",
        "\n",
        "# Ausgabe der Korrelationen\n",
        "for comparison, correlation in correlations.items():\n",
        "    print(f\"Die Korrelation zwischen {comparison} beträgt: {correlation:.4f}\")\n",
        "\n",
        "# Exportiere die Korrelationsdaten\n",
        "correlations_df = pd.DataFrame(list(correlations.items()), columns=['Vergleich', 'Korrelation'])\n",
        "correlations_df.to_csv('aehnlichkeitsmatrix_korrelationen_2.csv', index=False)\n",
        "\n",
        "### ACHTUNG: Es wird Pearson-Korrelation verwendet, um die Matrizen zu korrelieren\n",
        "### (Es wird bewertet, ob hohe (bzw. niedrige) Kosinus-Ähnlichkeitswerte in einer Matrix auch hohe (bzw. niedrige) Werte in der anderen Matrix haben)\n",
        "\n",
        "\n",
        "#################################################################\n",
        "### Schritt 2: a) Lineare Dimensionsreduktion mit PCA         ###\n",
        "###            b) Ähnlichkeit zwischen Vogelbildern berechnen ###\n",
        "###            c) Ergebnisse in Ähnlichkeitsmatrix speichern  ###\n",
        "#################################################################\n",
        "\n",
        "import os                                               # (kann man weglassen, wenn alles auf einmal gerunt wird)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image        # image-Modul enthält Funktionen zur Vorverarbeitung von Bildern zB. in ein CNN-taugliches Format\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # Zum Berechnen der Kosinus-Ähnlichkeit\n",
        "import numpy as np                                      # Zur Arbeit mit Arrays/Matrizen etc.\n",
        "import pandas as pd                                     # Zum Export von Daten in CSV-Dateien\n",
        "from sklearn.decomposition import PCA                   # PCA für die Dimensionsreduktion importieren\n",
        "\n",
        "ordner_pfad = '/content/Tierbilder' # Pfad zum Ordner festgelegt, in dem sich die Bilder befinden\n",
        "tierbilder = os.listdir(ordner_pfad) # Listet alle Dateien im Ordner ordner_pfad auf und speichert sie in der Liste \"tierbilder\"\n",
        "\n",
        "\n",
        "### Funktion zum Extrahieren von Features mit mehreren vortrainierten Keras-Modellen ###\n",
        "\n",
        "def extract_features_keras(model, preprocess_input, target_size, ordner_pfad):\n",
        "    # Definiert eine Funktion `extract_features_keras`, die die Merkmale von Bildern in einem Ordner extrahiert.\n",
        "    # Die Funktion nimmt vier Argumente entgegen: `model` (ein vortrainiertes Keras-Modell), `preprocess_input` (eine Funktion zur Bildvorverarbeitung),\n",
        "    # `target_size` (die Zielgröße für die Bildskalierung, z. B. (224, 224)) und `ordner_pfad` (der Pfad zum Ordner mit den Bildern)\n",
        "    dateien = sorted(os.listdir(ordner_pfad))     # Liest die Liste der Dateien im angegebenen `ordner_pfad` und sortiert sie alphabetisch\n",
        "    features = []\n",
        "    for datei in dateien:\n",
        "        bild_pfad = os.path.join(ordner_pfad, datei)\n",
        "        img = image.load_img(bild_pfad, target_size=target_size)\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = preprocess_input(img_array)\n",
        "        img_array = tf.expand_dims(img_array, axis=0)\n",
        "        features.append(model.predict(img_array))\n",
        "    return np.concatenate(features, axis=0)\n",
        "# Bilder werden geladen, skaliert, in numpy-arrays umgewandelt; mit preprocess.input normalisiert (= an jeweilige CNN angepasst)\n",
        "# Batch-Dimension wird mit tf.expand_dims zu jedem Bild hinzugefügt, damit jedes einzeln als Batch behandelt wird und nicht alle zusammen\n",
        "# Durchlauf durch die verschiedenen Modelle und Rückgabe der Vektoren\n",
        "\n",
        "\n",
        "### Erstellen eines Dictionarys aus Keras Modellen und deren Preprocessing-Funktionen etc. ###\n",
        "\n",
        "keras_models = {\n",
        "    'ResNet50': (tf.keras.applications.ResNet50(weights='imagenet', include_top=False, pooling='avg'),\n",
        "                 tf.keras.applications.resnet50.preprocess_input, (224, 224)),\n",
        "    'VGG16': (tf.keras.applications.VGG16(weights='imagenet', include_top=False, pooling='avg'),\n",
        "              tf.keras.applications.vgg16.preprocess_input, (224, 224)),\n",
        "    'InceptionV3': (tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, pooling='avg'),\n",
        "                    tf.keras.applications.inception_v3.preprocess_input, (299, 299)),\n",
        "    'DenseNet121': (tf.keras.applications.DenseNet121(weights='imagenet', include_top=False, pooling='avg'),\n",
        "                    tf.keras.applications.densenet.preprocess_input, (224, 224))\n",
        "}\n",
        "# Es wird jeweils das vortrainierte Modell mit den imagenet-Gewichten geladen;\n",
        "# include_top=False bedeutet, dass die oberen Schichten (Klassifikationsschicht) entfernt werden, sodass nur die Convolutional-Layer (Merkmalextraktionsschichten) übrig bleiben\n",
        "# pooling='avg' fügt eine durchschnittliche Pooling-Schicht hinzu, um den Ausgabe-Feature-Vektor zu reduzieren\n",
        "# Danach kommt die spezifische Vorverarbeitungsfunktion und die Zielgröße der Bilder\n",
        "\n",
        "\n",
        "### Extrahieren und Speichern der Feature-Embeddings und Erstellen von Ähnlichkeitsmatrizen ###\n",
        "\n",
        "# Iteration über das Dictionary mit der zuvor definierten Funktion zur Feature-Extraktion\n",
        "for model_name, (model, preprocess_input, target_size) in keras_models.items():\n",
        "    features = extract_features_keras(model, preprocess_input, target_size, ordner_pfad)\n",
        "\n",
        "    # Dimensionsreduktion mit PCA\n",
        "    pca = PCA(n_components=2)  # PCA initialisieren\n",
        "    reduced_features = pca.fit_transform(features)  # PCA anwenden\n",
        "\n",
        "    # Export der Feature-Embeddings in eine CSV-Datei\n",
        "    features_df = pd.DataFrame(features)\n",
        "    features_df.to_csv(f'reduced_embeddings_{model_name}_2.csv', index=False)\n",
        "\n",
        "    # Berechnen der Ähnlichkeitsmatrix (Berechnet für jeden Merkmalsvektor im Array features die Kosinus-Ähnlichkeit zu jedem anderen Vektor)\n",
        "    similarity_matrix_2 = cosine_similarity(features)\n",
        "\n",
        "    # Export der Ähnlichkeitsmatrix in eine CSV-Datei\n",
        "    df = pd.DataFrame(similarity_matrix_2)\n",
        "    df.to_csv(f'reduced_similarity_matrix_{model_name}_2.csv', index=False)\n",
        "\n",
        "    print(f\"{model_name} reduced similarity matrix:\") # Kann man weglassen, ist nur zum Prüfen des aktuellen Stands und Anschauen der Matrizen\n",
        "    print(similarity_matrix_2)\n",
        "\n",
        "\n",
        "### ACHTUNG: Hier wird Kosine-Korrelation verwendet, um zu beurteilen, wie ähnlich zwei Bilder im Merkmalsraum sind, unabhängig von der absoluten Größe der Merkmalsvektoren.\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "### Schritt 3: Korrelieren der Ähnlichkeitsmatrizen untereinander ###\n",
        "#####################################################################\n",
        "\n",
        "# Wieder Dictionary erstellen zum Zugreifen auf die CSVs\n",
        "similarity_matrices = {\n",
        "    'ResNet50': pd.read_csv('reduced_similarity_matrix_ResNet50_2.csv'),\n",
        "    'VGG16': pd.read_csv('reduced_similarity_matrix_VGG16_2.csv'),\n",
        "    'InceptionV3': pd.read_csv('reduced_similarity_matrix_InceptionV3_2.csv'),\n",
        "    'DenseNet121': pd.read_csv('reduced_similarity_matrix_DenseNet121_2.csv')\n",
        "}\n",
        "\n",
        "# Korrelation der Ähnlichkeitsmatrizen berechnen\n",
        "correlations = {}\n",
        "for name1, mat1 in similarity_matrices.items():\n",
        "    for name2, mat2 in similarity_matrices.items():\n",
        "        if name1 < name2:                     # Sicherstellen, dass jedes Modellpaar nur einmal betrachtet wird\n",
        "            mat2 = mat2.set_index(mat1.index) # Sicherstellen, dass die beiden Ähnlichkeitsmatrizen mat1 und mat2 denselben Index haben\n",
        "            corr = mat1.stack().corr(mat2.stack())\n",
        "            correlations[f'{name1} vs {name2}'] = corr\n",
        "\n",
        "# Ausgabe der Korrelationen\n",
        "for comparison, correlation in correlations.items():\n",
        "    print(f\"Die Korrelation zwischen {comparison} beträgt: {correlation:.4f}\")\n",
        "\n",
        "# Exportiere die Korrelationsdaten\n",
        "correlations_df = pd.DataFrame(list(correlations.items()), columns=['Vergleich', 'Korrelation'])\n",
        "correlations_df.to_csv('reduced_aehnlichkeitsmatrix_korrelationen_2.csv', index=False)\n",
        "\n",
        "### ACHTUNG: Es wird Pearson-Korrelation verwendet, um die Matrizen zu korrelieren\n",
        "### (Es wird bewertet, ob hohe (bzw. niedrige) Kosinus-Ähnlichkeitswerte in einer Matrix auch hohe (bzw. niedrige) Werte in der anderen Matrix haben)\n",
        "\n",
        "\n",
        "'''\n",
        "######################################################################################\n",
        "### Schritt 3: a) Berechnen des Durchschnitts menschlicher Ähnlichkeitsratings     ###\n",
        "###            b) Vergleich zwischen Modellen und menschlicher Durchschnittsmatrix ###\n",
        "###            c) Vergleich zwischen Modell und einzelnen menschlichen Ratings     ###\n",
        "###            d) Vergleich der menschlichen Ratings untereinander                 ###\n",
        "######################################################################################\n",
        "###                                                                                ###\n",
        "###    ->  DIE MENSCHENURTEILE GIBT ES BISLANG NOCH NICHT !!!                      ###\n",
        "###                                                                                ###\n",
        "### Sobald sie da sind, kann man einen Ordner \"Menschenunrteile_2\" in Google Colab ###\n",
        "### erstellen, die Urteile darin hochladen und dann den Code einfach komplett      ###\n",
        "### laufen lassen.                                                                 ###\n",
        "###                                                                                ###\n",
        "######################################################################################\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr # Das kann man alles weglassen, wenn der ganze Codenblock auf einmal runt\n",
        "\n",
        "# Pfad zum Ordner mit den menschlichen Ähnlichkeitsratings\n",
        "menschen_ordner_pfad = '/content/Menschenurteile_2'\n",
        "model_ordner_pfad = '/content/'\n",
        "\n",
        "# Lade menschliche Ähnlichkeitsratings aus dem Ordner\n",
        "def load_human_ratings(menschen_ordner_pfad):\n",
        "    dateien = sorted(os.listdir(menschen_ordner_pfad))\n",
        "    human_ratings = []\n",
        "    for datei in dateien:\n",
        "        if datei.endswith('.csv'):\n",
        "            pfad = os.path.join(menschen_ordner_pfad, datei)\n",
        "            df = pd.read_csv(pfad)\n",
        "            human_ratings.append(df.values)  # CSV in ein Numpy-Array umwandeln\n",
        "    return human_ratings, dateien\n",
        "\n",
        "# Berechne den Durchschnitt der menschlichen Ähnlichkeitsratings\n",
        "def average_human_ratings(human_ratings):\n",
        "    return np.mean(human_ratings, axis=0)\n",
        "\n",
        "# Lade DNN-Ähnlichkeitsmatrix\n",
        "def load_model_similarity_matrix(model_name, model_ordner_pfad):\n",
        "    pfad = os.path.join(model_ordner_pfad, f'similarity_matrix_{model_name}_2.csv')\n",
        "    return pd.read_csv(pfad).values\n",
        "\n",
        "# Berechne die Pearson-Korrelation zwischen zwei Matrizen\n",
        "# Zuerst werden die Matrizen in eindimensionale Arrays umgewandelt, und dann wird die Korrelation berechnet\n",
        "def calculate_correlation(matrix1, matrix2):\n",
        "    matrix1_flat = matrix1.flatten()\n",
        "    matrix2_flat = matrix2.flatten()\n",
        "    correlation, _ = pearsonr(matrix1_flat, matrix2_flat)\n",
        "    return correlation\n",
        "\n",
        "# Erstellen und Speichern der durchschnittlichen menschlichen Matrix in eine CSV-Datei\n",
        "human_ratings, human_filenames = load_human_ratings(menschen_ordner_pfad)\n",
        "average_human_matrix = average_human_ratings(human_ratings)\n",
        "pd.DataFrame(average_human_matrix).to_csv('average_human_ratings_2.csv', index=False)\n",
        "\n",
        "# Lade die Ähnlichkeitsmatrizen der Modelle\n",
        "model_names = ['ResNet50', 'VGG16', 'InceptionV3', 'DenseNet121']\n",
        "model_correlations = {}\n",
        "human_model_correlations = {}\n",
        "\n",
        "# Vergleiche jedes Modell mit den durchschnittlichen menschlichen Ähnlichkeitsratings\n",
        "for model_name in model_names:\n",
        "    model_matrix = load_model_similarity_matrix(model_name, model_ordner_pfad)\n",
        "\n",
        "    # Korrelation zwischen Modell und durchschnittlicher menschlicher Matrix\n",
        "    correlation_avg = calculate_correlation(model_matrix, average_human_matrix)\n",
        "    model_correlations[model_name] = correlation_avg\n",
        "    print(f\"Korrelation zwischen {model_name} und den durchschnittlichen menschlichen Ratings: {correlation_avg:.4f}\") # Aktuellen Stand checken\n",
        "\n",
        "    # Korrelation zwischen Modell und einzelnen menschlichen Ratings\n",
        "    for i, human_matrix in enumerate(human_ratings):\n",
        "        correlation = calculate_correlation(model_matrix, human_matrix)\n",
        "        human_model_correlations[f'{human_filenames[i]} vs {model_name}'] = correlation\n",
        "        print(f\"Korrelation zwischen {model_name} und {human_filenames[i]}: {correlation:.4f}\") # Aktuellen Stand checken\n",
        "\n",
        "# Vergleiche die menschlichen Ratings untereinander\n",
        "human_correlations = {}\n",
        "for i, human_matrix1 in enumerate(human_ratings):\n",
        "    for j, human_matrix2 in enumerate(human_ratings):\n",
        "        if i < j:\n",
        "            correlation = calculate_correlation(human_matrix1, human_matrix2)\n",
        "            human_correlations[f'Person {i+1} vs Person {j+1}'] = correlation\n",
        "            print(f\"Korrelation zwischen Person {i+1} und Person {j+1}: {correlation:.4f}\") # Aktuellen Stand checken\n",
        "\n",
        "# Speichern der Korrelationen in CSV-Dateien\n",
        "pd.DataFrame(list(model_correlations.items()), columns=['Modell', 'Korrelation']).to_csv('model_vs_human_average_correlations_2.csv', index=False)\n",
        "pd.DataFrame(list(human_model_correlations.items()), columns=['Vergleich', 'Korrelation']).to_csv('human_vs_model_correlations_2.csv', index=False)\n",
        "pd.DataFrame(list(human_correlations.items()), columns=['Vergleich', 'Korrelation']).to_csv('human_correlations_2.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "####################################################################################\n",
        "### Schritt 5: Tabellarische Gegenüberstellung der einzelnen Ähnlichkeitsratings ###\n",
        "####################################################################################\n",
        "\n",
        "# Erstellen einer Liste mit allen Modellnamen und menschlichen Bewertungen (inklusive Average)\n",
        "all_names = model_names + ['Mensch-Average'] + human_filenames\n",
        "\n",
        "# Initialisieren einer leeren DataFrame für die Korrelationsmatrix\n",
        "correlation_table = pd.DataFrame(index=all_names, columns=all_names)\n",
        "\n",
        "# Vergleiche Modelle untereinander, den Mensch-Average und die einzelnen menschlichen Bewertungen\n",
        "for name1 in all_names:\n",
        "    # Lade die entsprechende Ähnlichkeitsmatrix für name1\n",
        "    if name1 in model_names:\n",
        "        matrix1 = load_model_similarity_matrix(name1, model_ordner_pfad)\n",
        "    elif name1 == 'Mensch-Average':\n",
        "        matrix1 = average_human_matrix\n",
        "    else:\n",
        "        matrix1 = human_ratings[human_filenames.index(name1)]\n",
        "\n",
        "    # Vergleiche name1 mit allen anderen (Modellen, Menschen, Average)\n",
        "    for name2 in all_names:\n",
        "        # Lade die entsprechende Ähnlichkeitsmatrix für name2\n",
        "        if name2 in model_names:\n",
        "            matrix2 = load_model_similarity_matrix(name2, model_ordner_pfad)\n",
        "        elif name2 == 'Mensch-Average':\n",
        "            matrix2 = average_human_matrix\n",
        "        else:\n",
        "            matrix2 = human_ratings[human_filenames.index(name2)]\n",
        "\n",
        "        # Berechne die Korrelation zwischen matrix1 und matrix2\n",
        "        correlation = calculate_correlation(matrix1, matrix2)\n",
        "\n",
        "        # Füge die Korrelation in die DataFrame ein\n",
        "        correlation_table.loc[name1, name2] = correlation\n",
        "\n",
        "# Speichern der Korrelationsmatrix als CSV-Datei\n",
        "correlation_table.to_csv('correlation_matrix_table_2.csv', index=True)\n",
        "\n",
        "print(\"Task completed\")\n",
        "'''\n",
        "\n"
      ]
    }
  ]
}